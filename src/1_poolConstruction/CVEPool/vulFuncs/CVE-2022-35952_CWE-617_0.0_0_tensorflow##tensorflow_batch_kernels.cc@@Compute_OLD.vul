
    const Tensor& data_t = context->input(0);
    const Tensor& batch_index_t = context->input(1);
    const Tensor& grad_t = context->input(2);

    mutex_lock ml(mu_);

    const int64_t batch_key = context->input(3).scalar<int64_t>()();
    // Mark our tensor as available.
    if (!available_tensors_.emplace(batch_key, grad_t).second) 
      return errors::InvalidArgument("Two runs with the same batch key.");
    

    // Check whether we have a valid input tensor and, if so, create its
    // dispatch logic.
    if (data_t.NumElements() > 0) 
      if (batch_index_t.NumElements() == 0) 
        return errors::InvalidArgument(
            "batch_index is empty while the tensor isn't.");
      
      std::unordered_set<int64_t> missing_tensors;
      const auto batch_index =
          batch_index_t.shaped<int64_t, 2>(batch_index_t.dim_size(0), 3);
      for (int i = 0; i < batch_index_t.dim_size(0); ++i) 
        const int64_t batch_key = batch_index(i, 0);
        if (available_tensors_.find(batch_key) == available_tensors_.end()) 
          missing_tensors.emplace(batch_key);
        
      
      if (missing_tensors.empty()) 
        return OutputBatch(context, done);
      
      if (!available_batches_
               .emplace(batch_key, Batchmissing_tensors, context, done)
               .second) 
        return errors::InvalidArgument(
            "Batch key with valid batch used twice.");
      
      for (const int64_t i : missing_tensors) 
        if (!desired_tensor_to_batch_map_.emplace(i, batch_key).second) 
          return errors::InvalidArgument(
              "Missing tensor wanted by more than one batch.");
        
      
     else 
      // If we don't have a valid input tensor we can output an empty tensor and
      // call our done closure.
      TensorShape output_shape(grad_t.shape());
      output_shape.set_dim(0, 0);
      Tensor* output = nullptr;
      TF_RETURN_IF_ERROR(context->allocate_output(0, output_shape, &output));
      done();
    

    // Search to see whether our tensor is desired by any existing batch.
    auto desire_it = desired_tensor_to_batch_map_.find(batch_key);
    if (desire_it != desired_tensor_to_batch_map_.end()) 
      // Mark our tensor as no longer missing.
      auto batch_it = available_batches_.find(desire_it->second);
      desired_tensor_to_batch_map_.erase(desire_it);
      if (batch_it == available_batches_.end()) 
        return errors::InvalidArgument("Batch no longer exists.");
      
      batch_it->second.missing_tensors.erase(batch_key);
      // If all tensors are available we should concatenate them and dispatch
      // the batch.
      if (batch_it->second.missing_tensors.empty()) 
        TF_RETURN_IF_ERROR(
            OutputBatch(batch_it->second.context, batch_it->second.done));
        available_batches_.erase(batch_it);
      
    
    return OkStatus();
  